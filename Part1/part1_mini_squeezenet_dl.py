# -*- coding: utf-8 -*-
"""Part1_Mini_SqueezeNet_DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JdN4xneIH9mdSs7QOUu4ZJy_HIqxu5lN
"""

from keras.models import Model
from keras.layers import Activation,Concatenate,GlobalAveragePooling2D, BatchNormalization,Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, concatenate
from keras.utils import plot_model
from keras.optimizers import Adam
from keras.callbacks import LearningRateScheduler, ModelCheckpoint
import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd
from datetime import datetime
from tqdm import tqdm
from imutils import paths
from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report
from keras.utils import to_categorical


# decy function
def exponential_decay(step):
  initial_learning_rate = 0.001
  decay_steps= 100000
  decay_rate=0.2
  return initial_learning_rate * decay_rate ** (step / decay_steps)

def step_decay(curr_epoch):
  init_eta = 0.001 
  drop_factor = 0.2
  drop_every = 10 
  decay = np.floor((1 + curr_epoch) / drop_every) 
  eta = init_eta * drop_factor ** decay  
  return eta



def plot_training_loss_data_generator(H, N, plotPath):   
  plt.style.use("ggplot")
  plt.figure()
  plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
  plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
  plt.plot(np.argmax(H.history["loss"]), np.min(H.history["loss"]), marker="x",color="r", label="meilleur train_loss")
  plt.title("Training loss")
  plt.xlabel("Epoch #")
  plt.ylabel("loss")
  plt.legend(loc="lower left")
  plt.savefig(plotPath)
  plt.show()

def plot_training_accu_data_generator(H, N, plotPath):   
  plt.style.use("ggplot")
  plt.figure()
  plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
  plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
  plt.plot(np.argmax(H.history["val_accuracy"]), np.max(H.history["val_accuracy"]), marker="x",color="g", label="meilleur accuracy")
  plt.title("Training Accuracy")
  plt.xlabel("Epoch #")
  plt.ylabel("Accuracy")
  plt.legend(loc="lower left")
  plt.savefig(plotPath)
  plt.show()

num_classes = 101
#Création de CNN
def build_model ():
   inputs = Input(shape=(224,224,3)) 
   conv1 = Conv2D(96, kernel_size=7, padding='same',strides=2, activation='relu')(inputs)
   maxpool1 = MaxPooling2D(pool_size=(3,3),strides=2)(conv1)
   
   #1er Fire Module
   fire1_squeeze = Conv2D(16, kernel_size=1, padding='same',activation='relu')(maxpool1)
   fire1_expand1 = Conv2D(64, kernel_size=1, padding='same',activation='relu')(fire1_squeeze)
   fire1_expand2 = Conv2D(64, kernel_size=3, padding='same',activation='relu')(fire1_squeeze)
   concatenate_1 = concatenate([fire1_expand1, fire1_expand2])
   
   #2eme Fire Module
   fire2_squeeze = Conv2D(16, kernel_size=1, padding='same',activation='relu')(concatenate_1)
   fire2_expand1 = Conv2D(64, kernel_size=1, padding='same',activation='relu')(fire2_squeeze)
   fire2_expand2 = Conv2D(64, kernel_size=3, padding='same',activation='relu')(fire2_squeeze)
   concatenate_2= concatenate([fire2_expand1, fire2_expand2])
   
   # 3eme Fire Module
   fire3_squeeze = Conv2D(32, kernel_size=1, padding='same',activation='relu')(concatenate_2)
   fire3_expand1 = Conv2D(128, kernel_size=1, padding='same',activation='relu')(fire3_squeeze)
   fire3_expand2 = Conv2D(128, kernel_size=3, padding='same',activation='relu')(fire3_squeeze)
   concatenate_3= concatenate([fire3_expand1, fire3_expand2])
   
   # max pooling
   maxpool2 = MaxPooling2D(pool_size=(3,3),strides=2)(concatenate_3)
   
   # 4eme Fire Module
   fire4_squeeze = Conv2D(32, kernel_size=1, padding='same',activation='relu')(maxpool2)
   fire4_expand1 = Conv2D(128, kernel_size=1, padding='same',activation='relu')(fire4_squeeze)
   fire4_expand2 = Conv2D(128, kernel_size=3, padding='same',activation='relu')(fire4_squeeze)
   concatenate_4= concatenate([fire4_expand1, fire4_expand2])
   
   #FC
   dropout_1 = Dropout(0.5)(concatenate_4)
   conv2 = Conv2D(num_classes, kernel_size=1, padding='valid', activation='relu')(dropout_1)
   global_average_pooling2d_1 = GlobalAveragePooling2D()(conv2)
   softmax  = Activation('softmax')(global_average_pooling2d_1)

   #Compilation
   model = Model(inputs=inputs, outputs=softmax)
   
   #Visualiser le modèle
   #print(model.summary())
   
   return model

   
#Définir la varible globale
BASE_PATH = "DataSet_Bark101"

#Définir les noms des repertoires train et test
TRAIN = "Bark101_train"
TEST  = "Bark101_test"

#Labels des classes
CLASSES = range(0,102)
CLASSES = [str(i) for i in CLASSES]

#Taille du batch 
BATCH_SIZE = 64
epoch = 100

#Les chemins vers les repertoires train et test
trainPath = os.path.sep.join([BASE_PATH, TRAIN])
testPath = os.path.sep.join([BASE_PATH, TEST])

#Le nombre total des ilages dans chacun des répétoires train et test
totalTrain = len(list(paths.list_images(trainPath)))
print(totalTrain)

totalTest = len(list(paths.list_images(testPath)))
print(totalTest)

#Instanciation d'un objet ImageDataGenerator pour l'augmentation des données train
trainAug = ImageDataGenerator(
    horizontal_flip= True,
    fill_mode = "nearest")


#Instanciation d'un objet ImageDataGenerator pour l'augmentation des données test
testAug = ImageDataGenerator()

#Initialiser le generateur de train
trainGen = trainAug.flow_from_directory(
    trainPath,
    class_mode="categorical",
    target_size=(224, 224),
    color_mode="rgb",
    shuffle=True,
    batch_size=BATCH_SIZE
    )

#Initialiser le generateur de test
testGen = testAug.flow_from_directory(
    testPath,
    class_mode="categorical",
    target_size=(224, 224),
    color_mode="rgb",
    shuffle=False,
    batch_size=BATCH_SIZE)

#Entrainement du modèle
model = build_model()
# compiler le modele
adam = Adam(learning_rate=0.001)

filepath="best_model.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,
              save_best_only=True, mode='max')
learningRateScheduler = LearningRateScheduler(exponential_decay, verbose=1)
callbacks = [learningRateScheduler, checkpoint]
model.compile(loss='categorical_crossentropy', optimizer= adam, metrics=['accuracy'])
H = model.fit_generator(
    trainGen,
    steps_per_epoch=totalTrain // BATCH_SIZE,
    validation_data=testGen,
    validation_steps=totalTest // BATCH_SIZE,
    epochs=100, 
    callbacks= callbacks
)

#save the history of training

history = pd.DataFrame(H.history)
epoch = 100
now = datetime.now()
#time to str
dt_string = now.strftime("%d_%m_%Y_%H:%M:%S")
file_path = "history_of_trainig_"+str(BATCH_SIZE)+"_"+str(epoch)+"_"+dt_string+".json"
# save in json file
with open(file_path, mode="w") as fh:
  history.to_json(fh)


#Afficher le rapport de classification le modele optimal sur l ensemble test
testGen.reset()
prediction = model.predict_generator(testGen)
true_y = to_categorical(testGen.labels, num_classes=101, dtype="float32")
classification_reporo = classification_report(np.argmax(true_y, axis=1), np.argmax(prediction, axis=1), labels=CLASSES, output_dict=True)

#Save the classification_report to a json file
df = pd.DataFrame(classification_reporo)
with open("report.json", 'wb') as fh:
  df.to_json("classification_report.json")

#display report
print(classification_reporo)


#afficher history de training
plot_training_loss_data_generator(H, 100, "loss.png")
plot_training_accu_data_generator(H, 100, "accuracy.png")